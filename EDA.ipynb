{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project: Nautral Language Processing on Wine Reviews \n",
    "Maria Corina Cabezas, Austin Doolitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/austin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/austin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import collections\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from yellowbrick.regressor import PredictionError\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Rows: 129971\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...   \n",
       "3        US  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4        US  Much like the regular bottling from 2012, this...   \n",
       "\n",
       "                          designation  points  price           province  \\\n",
       "0                        Vulkà Bianco      87    NaN  Sicily & Sardinia   \n",
       "1                            Avidagos      87   15.0              Douro   \n",
       "2                                 NaN      87   14.0             Oregon   \n",
       "3                Reserve Late Harvest      87   13.0           Michigan   \n",
       "4  Vintner's Reserve Wild Child Block      87   65.0             Oregon   \n",
       "\n",
       "              region_1           region_2         taster_name  \\\n",
       "0                 Etna                NaN       Kerin O’Keefe   \n",
       "1                  NaN                NaN          Roger Voss   \n",
       "2    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "3  Lake Michigan Shore                NaN  Alexander Peartree   \n",
       "4    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "\n",
       "  taster_twitter_handle                                              title  \\\n",
       "0          @kerinokeefe                  Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1            @vossroger      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2           @paulgwine       Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3                   NaN  St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4           @paulgwine   Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "\n",
       "          variety               winery  \n",
       "0     White Blend              Nicosia  \n",
       "1  Portuguese Red  Quinta dos Avidagos  \n",
       "2      Pinot Gris            Rainstorm  \n",
       "3        Riesling           St. Julian  \n",
       "4      Pinot Noir         Sweet Cheeks  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('./data/winemag-data-130k-v2.csv', index_col=0)\n",
    "print(f'N Rows: {len(df.index)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of duplicates. I noticed that simply running data.drop_duplicates() did not remove all of them, because some column values were slightly different. Nevertheless, the fact that the \"Description\" columns were identical was sufficient to determine these were not different reviews. I decided to drop all duplicates based on the description column alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Rows: 119955\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates('description')\n",
    "print(f'N Rows: {len(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of missing values on each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                     59\n",
       "description                  0\n",
       "designation              34532\n",
       "points                       0\n",
       "price                     8388\n",
       "province                    59\n",
       "region_1                 19558\n",
       "region_2                 73195\n",
       "taster_name              24912\n",
       "taster_twitter_handle    29441\n",
       "title                        0\n",
       "variety                      1\n",
       "winery                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to delete the columns that have a very large number of missing values like designation, region_2 and taster_twitter_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['designation','region_2','taster_twitter_handle']\n",
    "df = df.drop(columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also noticed that price has 8388 missing values, so we will replace these with the average price of wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/jupyter/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# TODO Imputing is mostly meant for features, not for the target values.\n",
    "# we should probably just drop missing values\n",
    "\n",
    "imp=Imputer(missing_values=\"NaN\", strategy=\"mean\" )\n",
    "df[\"price\"]=imp.fit_transform(df[[\"price\"]]).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Dev Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we split our dataset into 3 sets, namely training set, dev set, and test set. This is so that we can be more confident at our models and to better compare the model options we come up with. Specifically, training set is used to train our models, dev set is used to optimize each model, and test set is used to evaluate performance of the model. We have assigned 20% of the total dataset to be the test set, 20% to be the dev set, and the rest being training set. We made the split here because it will be used in the Data Exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (71973, 10)\n",
      "Dev shape: (23991, 10)\n",
      "Test shape: (23991, 10)\n"
     ]
    }
   ],
   "source": [
    "# split it into train, dev, and test\n",
    "# courtesy of https://stackoverflow.com/a/38251063\n",
    "perm = np.random.permutation(df.index)\n",
    "m = len(df.index)\n",
    "\n",
    "train_percent = .6\n",
    "dev_percent = .2\n",
    "\n",
    "train_end = int(m * train_percent)\n",
    "dev_end = int(m * dev_percent) + train_end\n",
    "\n",
    "train_df = df.loc[perm[:train_end]]\n",
    "dev_df = df.loc[perm[train_end:dev_end]]\n",
    "test_df = df.loc[perm[dev_end:]]\n",
    "\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Dev shape: {dev_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Investigation\n",
    "Let's start by just looking at the data and seeing what pops out to us. We'll spend a little time looking at the continuous values points and price, and then spend more time looking at the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>71934</td>\n",
       "      <td>71973</td>\n",
       "      <td>71973.000000</td>\n",
       "      <td>71973.000000</td>\n",
       "      <td>71934</td>\n",
       "      <td>60248</td>\n",
       "      <td>56995</td>\n",
       "      <td>71973</td>\n",
       "      <td>71972</td>\n",
       "      <td>71973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>41</td>\n",
       "      <td>71973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>387</td>\n",
       "      <td>1135</td>\n",
       "      <td>19</td>\n",
       "      <td>71515</td>\n",
       "      <td>627</td>\n",
       "      <td>14262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>US</td>\n",
       "      <td>Big but polished, this full-bodied wine almost...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Gloria Ferrer NV Sonoma Brut Sparkling (Sonoma...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Wines &amp; Winemakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>30183</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20167</td>\n",
       "      <td>2536</td>\n",
       "      <td>14186</td>\n",
       "      <td>8</td>\n",
       "      <td>7319</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.444625</td>\n",
       "      <td>35.712472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.094782</td>\n",
       "      <td>40.882064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                                        description  \\\n",
       "count    71934                                              71973   \n",
       "unique      41                                              71973   \n",
       "top         US  Big but polished, this full-bodied wine almost...   \n",
       "freq     30183                                                  1   \n",
       "mean       NaN                                                NaN   \n",
       "std        NaN                                                NaN   \n",
       "min        NaN                                                NaN   \n",
       "25%        NaN                                                NaN   \n",
       "50%        NaN                                                NaN   \n",
       "75%        NaN                                                NaN   \n",
       "max        NaN                                                NaN   \n",
       "\n",
       "              points         price    province     region_1 taster_name  \\\n",
       "count   71973.000000  71973.000000       71934        60248       56995   \n",
       "unique           NaN           NaN         387         1135          19   \n",
       "top              NaN           NaN  California  Napa Valley  Roger Voss   \n",
       "freq             NaN           NaN       20167         2536       14186   \n",
       "mean       88.444625     35.712472         NaN          NaN         NaN   \n",
       "std         3.094782     40.882064         NaN          NaN         NaN   \n",
       "min        80.000000      4.000000         NaN          NaN         NaN   \n",
       "25%        86.000000     18.000000         NaN          NaN         NaN   \n",
       "50%        88.000000     28.000000         NaN          NaN         NaN   \n",
       "75%        91.000000     40.000000         NaN          NaN         NaN   \n",
       "max       100.000000   2500.000000         NaN          NaN         NaN   \n",
       "\n",
       "                                                    title     variety  \\\n",
       "count                                               71973       71972   \n",
       "unique                                              71515         627   \n",
       "top     Gloria Ferrer NV Sonoma Brut Sparkling (Sonoma...  Pinot Noir   \n",
       "freq                                                    8        7319   \n",
       "mean                                                  NaN         NaN   \n",
       "std                                                   NaN         NaN   \n",
       "min                                                   NaN         NaN   \n",
       "25%                                                   NaN         NaN   \n",
       "50%                                                   NaN         NaN   \n",
       "75%                                                   NaN         NaN   \n",
       "max                                                   NaN         NaN   \n",
       "\n",
       "                    winery  \n",
       "count                71973  \n",
       "unique               14262  \n",
       "top     Wines & Winemakers  \n",
       "freq                   122  \n",
       "mean                   NaN  \n",
       "std                    NaN  \n",
       "min                    NaN  \n",
       "25%                    NaN  \n",
       "50%                    NaN  \n",
       "75%                    NaN  \n",
       "max                    NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points\n",
    "\n",
    "It appears that the points are located between 80-100. This matches up with the source of the data that claims they do not publish reviews for any wine scored less than 80. More information on wine scoring can be viewed [here](https://www.winespectator.com/articles/scoring-scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.distplot(train_df.points, bins=21, kde_kws={'bw':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like this is a textbook normal distribution with an incredibly slight left skew. Let's just make sure that every value is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(train_df.points.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price\n",
    "\n",
    "Since this is related to currency, I bet my lunch this is a heavy tailed distribution. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Missing values: {train_df.price.isna().sum()}')\n",
    "\n",
    "# let's remove the missing prices for now\n",
    "sns.distplot(train_df[~train_df.price.isna()].price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice... lets log scale this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['price_log'] = np.log(train_df.price)\n",
    "ax = sns.distplot(train_df[~train_df.price_log.isna()].price_log, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, I see that the lowest price is 4 dollars... what's the score on those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.price == 4].points.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cont_columns = train_df.columns[train_df.dtypes == np.object]\n",
    "print(f'Non-Continuous columns: {list(non_cont_columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by focusing on the descriptions. We'll tokenize, canonize, construct a vocabulary, and finally get counts for each of the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36322\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "\n",
    "for s in train_df.description:\n",
    "    s = s.lower()\n",
    "    tokenized = nltk.tokenize.word_tokenize(s)\n",
    "    counter.update(tokenized)\n",
    "\n",
    "vocab = counter.items()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view most common words \n",
    "most_common = counter.most_common(50)\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very interesting thing viewed here is that in the most common words are very wine specific words. This is pretty obvious in hindsight, but it made me laugh to see wine just outside of the top 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to see a wordcloud view of the most common words excluding punctuations and stopwords. We also excluded other common words like drink, wine, flavors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(review for review in df.description)\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=[20,20])\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems black cherry and full bodied are the most common characteristics and Carbernet Sauvignon is the most discussed type of wine. Let's see if this is due to it being more common in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety_df = df.groupby('variety').filter(lambda x: len(x) > 2500)\n",
    "varieties = variety_df['variety'].value_counts().index.tolist()\n",
    "fig, ax = plt.subplots(figsize = (6, 4))\n",
    "sns.countplot(x = variety_df['variety'], order = varieties, ax = ax)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, Cabernet Sauvignon was the third most reviewed wine variety after Pinot Noir and Chardonnay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasters\n",
    "Now let's look at the tasters. We'll see how many there are, and the distribution of the number of reviews each has given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasters = train_df.taster_name.dropna().unique()\n",
    "print(f'{len(tasters)} Tasters: {list(tasters)}')\n",
    "train_df.taster_name = train_df.taster_name.astype('category')\n",
    "\n",
    "taster_counts  = train_df.taster_name.value_counts()\n",
    "ax = sns.countplot(train_df.taster_name, order=taster_counts.index)\n",
    "_ = ax.set_xticklabels(taster_counts.index, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there appears to be a Zeta distribution in the contributions of each taster. Let's what the numbers are on the 3 least active contributors: Carrie Dykes, Fiona Adams, and Christina Pickard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taster_counts[['Carrie Dykes', 'Fiona Adams', 'Christina Pickard']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to keep this in mind for subsequent analysis since the data related to their contributions will likely not be as representative as someone on the center or left of the distribution\n",
    "\n",
    "#### Taster Vocabulary\n",
    "Just for funsies, let's look at the vocabulary of each individual taster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taster_vocabs = {n: collections.Counter() for n in tasters}\n",
    "\n",
    "for _, row in train_df[~train_df.taster_name.isna()].iterrows():\n",
    "    taster = row.taster_name\n",
    "    s = row.description\n",
    "    s = s.lower()\n",
    "    tokenized = nltk.tokenize.word_tokenize(s)\n",
    "    taster_vocabs[taster].update(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taster_vocab_lens = {k: len(v.items()) for k, v in taster_vocabs.items()}\n",
    "taster_vocab_lens = pd.Series(taster_vocab_lens).sort_values().iloc[::-1]\n",
    "\n",
    "ax = sns.barplot(x=taster_vocab_lens.index, y=taster_vocab_lens)\n",
    "_ = ax.set_xticklabels(taster_vocab_lens.index, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Roger Voss is up top since he has had the most opportunity to use unique words. Let's compare the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_list = []\n",
    "\n",
    "contribution_count_list = list(taster_counts.index)\n",
    "\n",
    "print('Vocab Size compared to Contribution:')\n",
    "for vocab_idx, taster in enumerate(taster_vocab_lens.index):\n",
    "    count_idx = contribution_count_list.index(taster)\n",
    "    diff = count_idx - vocab_idx\n",
    "    print(f'\\t{vocab_idx + 1}: {taster} ({diff:+d})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerin O'Keefe appears to be relatively bland in her word usage. \n",
    "\n",
    "#### Taster Score Distribution\n",
    "Let's look at each reviewer's score distribution to make sure we don't have any biased reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 4\n",
    "rows = math.ceil(len(tasters) / float(cols))\n",
    "figs, axes = plt.subplots(rows, cols, figsize=(20,20))\n",
    "axes = axes.flatten()\n",
    "for taster, ax in zip(tasters, axes):\n",
    "    taster_reviews = train_df[train_df.taster_name == taster]\n",
    "    ax = sns.distplot(taster_reviews.points, ax=ax, bins=20)\n",
    "    ax.set_title(taster)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any bias immediately apparent for any of the reviewers, although this is difficult to ascertain due to the large range in the number of reviews per taster.\n",
    "\n",
    "#### Taster Price Distribution\n",
    "\n",
    "And now let's look at the distribution of the log price for each reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 4\n",
    "rows = math.ceil(len(tasters) / float(cols))\n",
    "figs, axes = plt.subplots(rows, cols, figsize=(20,20))\n",
    "axes = axes.flatten()\n",
    "for taster, ax in zip(tasters, axes):\n",
    "    taster_reviews = train_df[(train_df.taster_name == taster) & ~train_df.price_log.isna()]\n",
    "    ax = sns.distplot(taster_reviews.price_log, ax=ax, bins=20)\n",
    "    ax.set_title(taster)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Words\n",
    "Let's look at the top 3 unique words used by the tasters. We'll do this by iteratively constructing of the top 3 words used by each taster and removing any duplicate words across top 3 lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# collect the top 25 words from the corpus (not including nltk stop words)\n",
    "top_domain_specific_words = []\n",
    "n = 25\n",
    "for word, count in counter.most_common():\n",
    "    if word in stop_words:\n",
    "        continue\n",
    "    \n",
    "    top_domain_specific_words.append(word)\n",
    "    if len(top_domain_specific_words) == n:\n",
    "        break\n",
    "        \n",
    "all_stop_words = list(string.punctuation) + top_domain_specific_words + stop_words\n",
    "\n",
    "n = 3\n",
    "top_vocab = {}\n",
    "for taster_name in taster_vocabs:\n",
    "    taster_vocab = taster_vocabs[taster_name]\n",
    "    \n",
    "    unique_words = []\n",
    "    for word, count in taster_vocab.most_common():\n",
    "        if word in all_stop_words:\n",
    "            continue\n",
    "        \n",
    "        unique_words.append(word)\n",
    "        if len(unique_words) == n:\n",
    "            break\n",
    "        \n",
    "    top_vocab[taster_name] = unique_words\n",
    "        \n",
    "for taster_name, words in top_vocab.items():\n",
    "    print(f'{taster_name}: ')\n",
    "    for i, w in enumerate(words):\n",
    "        print(f'\\t{i+1}: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what words are most strongly correlated with price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_vec = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1,1),\n",
    "    min_df=10\n",
    ")\n",
    "X_score = score_vec.fit_transform(train_df.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(x, y):\n",
    "    y = np.expand_dims(y, axis=0)\n",
    "    x = x.toarray().T\n",
    "    points_np = np.concatenate([x, y], axis=0)\n",
    "    return np.corrcoef(points_np)[-1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_corr = get_corr(X_score, train_df.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_corr(corr, vec):\n",
    "    n = 10\n",
    "    corr_sorted_idx = np.argsort(corr)\n",
    "    vocab_words = list(vec.vocabulary_.keys())\n",
    "    \n",
    "    print('Highest correlations:')\n",
    "    for i, idx in enumerate(corr_sorted_idx[-1:-n:-1]):\n",
    "        corr_val = corr[idx]\n",
    "        word = vocab_words[idx]\n",
    "        print(f'\\t{i+1}. {word}: {corr_val}')\n",
    "\n",
    "    print('Lowest correlations:')\n",
    "    for i, idx in enumerate(corr_sorted_idx[:n]):\n",
    "        corr_val = corr[idx]\n",
    "        word = vocab_words[idx]\n",
    "        print(f'\\t{i+1}. {word}: {corr_val}')\n",
    "    \n",
    "    # plot the correlations\n",
    "    sorted_score_corr = score_corr[corr_sorted_idx]\n",
    "    _ = sns.distplot(sorted_score_corr)\n",
    "    \n",
    "display_corr(score_corr, score_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing but for price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_price_df = train_df.dropna(subset=['price'], axis=0, inplace=False)\n",
    "\n",
    "price_vec = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1,1),\n",
    "    min_df=10\n",
    ")\n",
    "X_price = price_vec.fit_transform(train_price_df.description)\n",
    "price_corr = get_corr(X_price, train_price_df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr(price_corr, price_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return math.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "def train_and_show(model, x, y, x_dev, y_dev):\n",
    "    visualizer = PredictionError(model)\n",
    "\n",
    "    visualizer.fit(x, y)  # Fit the training data to the visualizer\n",
    "    visualizer.score(x_dev, y_dev)  # Evaluate the model on the test data\n",
    "    visualizer.poof()\n",
    "    \n",
    "    y_pred = visualizer.estimator.predict(x_dev)\n",
    "    rmse_val = rmse(y_dev, y_pred)\n",
    "    print(f'Root MSE: {rmse_val:.3f}')\n",
    "    \n",
    "X_score_dev = score_vec.transform(dev_df.description)\n",
    "\n",
    "train_and_show(LinearRegression(), X_score, train_df.points, X_score_dev, dev_df.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_show(XGBRegressor(max_depth=5), X_score, train_df.points, X_score_dev, dev_df.points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = list(train_df['description'].astype(str))\n",
    "# y_train = train_df['variety'].astype(str)\n",
    "# X_dev = list(dev_df['description'].astype(str))\n",
    "# y_dev = dev_df['variety'].astype(str)\n",
    "# print(len(X_train), y_train.shape, len(X_dev), y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# train_featurevectors = vectorizer.fit_transform(X_train)\n",
    "# dev_featurevectors = vectorizer.transform(X_dev)\n",
    "\n",
    "# #hyperparameter tuning\n",
    "# param = {'n_neighbors': np.concatenate([np.arange(1,50,1),np.arange(50,100,2),np.arange(100,201,5)]).tolist()}\n",
    "# knn_bestparam = GridSearchCV(KNeighborsClassifier(), param, scoring='f1_macro')\n",
    "# knn_bestparam.fit(train_featurevectors, y_train)\n",
    "# optimal_k = knn_bestparam.best_params_['n_neighbors']\n",
    "\n",
    "# #model \n",
    "# knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "# knn.fit(train_featurevectors, y_train)\n",
    "# predictions = knn.predict(dev_featurevectors)\n",
    "# print('F1 score for a kNN classifier using k=90:', '{0:.4f}'.format(metrics.f1_score(y_dev,predictions, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train_df['region'].astype(str)\n",
    "# y_dev = dev_df['region'].astype(str)\n",
    "# print(len(X_train), y_train.shape, len(X_dev), y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# train_featurevectors = vectorizer.fit_transform(X_train)\n",
    "# dev_featurevectors = vectorizer.transform(X_dev)\n",
    "\n",
    "# #hyperparameter tuning\n",
    "# param = {'n_neighbors': np.concatenate([np.arange(1,50,1),np.arange(50,100,2),np.arange(100,201,5)]).tolist()}\n",
    "# knn_bestparam = GridSearchCV(KNeighborsClassifier(), param, scoring='f1_macro')\n",
    "# knn_bestparam.fit(train_featurevectors, y_train)\n",
    "# optimal_k = knn_bestparam.best_params_['n_neighbors']\n",
    "\n",
    "# #train the model with optimal hyperparameters\n",
    "# knn = KNeighborsClassifier(n_neighbors=90)\n",
    "# knn.fit(train_featurevectors, y_train)\n",
    "# predictions = knn.predict(dev_featurevectors)\n",
    "# print(\"Knn Classification report\")\n",
    "# print('F1 score for a kNN classifier using k=90:', '{0:.4f}'.format(metrics.f1_score(y_dev,predictions, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longest description, including start and end tokens\n",
    "max_len = 130\n",
    "\n",
    "def surround_texts(texts):\n",
    "    return [f'SSS {t} EEE' for t in texts]\n",
    "\n",
    "t = keras.preprocessing.text.Tokenizer()\n",
    "surrounded_train = surround_texts(train_df.description)\n",
    "t.fit_on_texts(surrounded_train)\n",
    "\n",
    "def get_dataset(data, label, batch_size):\n",
    "    surrounded_series = surround_texts(data)\n",
    "    seq = t.texts_to_sequences(surrounded_series)\n",
    "    pad =  keras.preprocessing.sequence.pad_sequences(\n",
    "        seq, \n",
    "        maxlen=max_len,\n",
    "    )\n",
    "    label_norm = (label - label.mean()) / label.std()\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((pad, label_norm))\n",
    "    dataset = dataset.shuffle().batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_vec = get_dataset(train_df.description, train_df.points)\n",
    "dev_vec = get_dataset(dev_df.description, dev_df.points)\n",
    "test_vec = get_dataset(test_df.description, test_df.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b710d76a42cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    344\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meeting Notes:\n",
    "- Add Price correlation with each word in vocab\n",
    "  - Make algo more memory efficient\n",
    "- Establish a baseline\n",
    "    - Binary Classifier\n",
    "    - Linear Regression on Highest and Lowest correlation words\n",
    "        - Price\n",
    "        - Points\n",
    "- Do same analysis on Bigrams/Trigrams/NGrams\n",
    "- Train new models\n",
    "    - CNN approach\n",
    "    - LSTM\n",
    "    - BERT\n",
    "    - Tasks?\n",
    "        - Price Regression\n",
    "        - Score Regression\n",
    "        - Region Classification\n",
    "        - Variety Classification\n",
    "        - Sentiment analysis by Winery\n",
    "- Appendix\n",
    "    - Country specific\n",
    "    - Region specific\n",
    "    - Taster specific"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
